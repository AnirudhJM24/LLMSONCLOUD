# RUN LLMS SUPPORTED ON OLLAMA ON THE CLOUD AND USE IT THROUGH YOUR LOCAL MACHINE

### Steps
- CREATE A NEW NOTEBOOK AND CHOOSE THE T4 RUNTIME
- CREATE THE server.py file on your local and upload it to your colab workspace
- COPY THE ollamaoncolab.ipynb NOTEBOOK INTO YOU COLAB NOTEBOOK AND RUN THE CELLS
- OPEN THE client.py ON YOUR MACHINE AND RUN IT 

### Future Plans
- Add a tutorial on pulling models from hugging face hub and host it on ollama + colab